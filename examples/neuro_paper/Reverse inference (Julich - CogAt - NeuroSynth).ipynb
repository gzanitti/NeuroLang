{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "naked-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gzanitti/.local/lib/python3.8/site-packages/scikit_learn-0.23.2-py3.8-macosx-10.9-x86_64.egg/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.feature_selection.univariate_selection module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_selection. Anything that cannot be imported from sklearn.feature_selection is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from nilearn import datasets, image\n",
    "from neurolang.frontend import NeurolangPDL\n",
    "from typing import Iterable\n",
    "from sklearn.model_selection import KFold\n",
    "from rdflib import RDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reduced-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 25\n",
    "resample = 4\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "thermal-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_region(elem, id_2_num, father=None, triples=[]):\n",
    "    name = elem['name']\n",
    "    if 'labelIndex' in elem:\n",
    "        if elem['labelIndex'] is not None:\n",
    "            index = int(elem['labelIndex'])\n",
    "            if index in id_2_num:\n",
    "                num = id_2_num[index]\n",
    "                triples.append((name, num))\n",
    "        \n",
    "    for c in elem['children']:\n",
    "        parse_region(c, id_2_num, father=name, triples=triples)\n",
    "        \n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "reasonable-summit",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Ontology\n",
    "julich_ontology_l = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('julich_ontology'),\n",
    "    [\n",
    "        (\n",
    "            'julich_ontology_l.xml',\n",
    "            'https://raw.githubusercontent.com/NeuroLang/neurolang_data/main/Julich-Brain/WB/22/MPM/'\n",
    "            'JulichBrain_MPMAtlas_l_N10_nlin2Stdicbm152asym2009c_publicDOI_3f6407380a69007a54f5e13f3c1ba2e6.xml',\n",
    "            {'move': 'julich_ontology_l.xml'}\n",
    "        )\n",
    "    ]\n",
    ")[0]\n",
    "\n",
    "julich_ontology_r = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('julich_ontology'),\n",
    "    [\n",
    "        (\n",
    "            'julich_ontology_r.xml',\n",
    "            'https://raw.githubusercontent.com/NeuroLang/neurolang_data/main/Julich-Brain/WB/22/MPM/'\n",
    "            'JulichBrain_MPMAtlas_l_N10_nlin2Stdicbm152asym2009c_publicDOI_3f6407380a69007a54f5e13f3c1ba2e6.xml',\n",
    "            {'move': 'julich_ontology_r.xml'}\n",
    "        )\n",
    "    ]\n",
    ")[0]\n",
    "\n",
    "jubrain_ontology = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('julich_ontology'),\n",
    "    [\n",
    "        (\n",
    "            'jubrain_ontology.xml',\n",
    "            'https://raw.githubusercontent.com/NeuroLang/neurolang_data/main/Julich-Brain/WB/22/jubrain-ontology_22.json',\n",
    "            {'move': 'jubrain_ontology.xml'}\n",
    "        )\n",
    "    ]\n",
    ")[0]\n",
    "\n",
    "tree = ET.parse(julich_ontology_l)\n",
    "\n",
    "id_2_num = {}\n",
    "for a in tree.iter():\n",
    "    if a.tag == 'Structure':\n",
    "        num = int(a.attrib['grayvalue'])\n",
    "        id_ = int(a.attrib['id'])\n",
    "        id_2_num[id_] = num\n",
    "\n",
    "tree = ET.parse(julich_ontology_r)\n",
    "\n",
    "for a in tree.iter():\n",
    "    if a.tag == 'Structure':\n",
    "        num = int(a.attrib['grayvalue'])\n",
    "        id_ = int(a.attrib['id'])\n",
    "        id_2_num[id_] = num\n",
    "\n",
    "\n",
    "with open(jubrain_ontology) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "regions = data['properties']['regions']\n",
    "for elem in regions:\n",
    "    triples = parse_region(elem, id_2_num)\n",
    "    \n",
    "    #for n, r in [\n",
    "    #    (13, 'GapMap Frontal-I (GapMap)'),\n",
    "    #    (32, 'GapMap Frontal-to-Occipital (GapMap)'),\n",
    "    #    (59, 'GapMap Temporal-to-Parietal (GapMap)'),\n",
    "    #    (89, 'GapMap Frontal-II (GapMap)'),\n",
    "    #    (95, 'GapMap Frontal-to-Temporal (GapMap)')\n",
    "    #]:\n",
    "    #    triples.append((r, n))\n",
    "        \n",
    "    f.close()   \n",
    "    regions = pd.DataFrame(triples, columns=['r_name', 'r_number']).astype({'r_number': 'int32'}).sort_values('r_number')\n",
    "    regions.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "under-universal",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions2 = regions.copy()\n",
    "regions2['r_number'] = regions2['r_number'] + 1000\n",
    "regions2['hemis'] = 'l'\n",
    "regions['hemis'] = 'r'\n",
    "\n",
    "regions = pd.concat((regions, regions2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aerial-finding",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Atlas\n",
    "mni_t1 = nib.load(datasets.fetch_icbm152_2009()['t1'])\n",
    "mni_t1_4mm = image.resample_img(mni_t1, np.eye(3) * resample)\n",
    "\n",
    "wb22_l = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('julich'),\n",
    "    [\n",
    "        (\n",
    "            'wb22_l.nii.gz',\n",
    "            'https://github.com/NeuroLang/neurolang_data/raw/main/Julich-Brain/WB/22/MPM/'\n",
    "            'JulichBrain_MPMAtlas_l_N10_nlin2Stdicbm152asym2009c_publicDOI_3f6407380a69007a54f5e13f3c1ba2e6.nii.gz',\n",
    "            {'move': 'wb22_l.nii.gz'}\n",
    "        )\n",
    "    ]\n",
    ")[0]\n",
    "\n",
    "wb22_r = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('julich'),\n",
    "    [\n",
    "        (\n",
    "            'wb22_r.nii.gz',\n",
    "            'https://github.com/NeuroLang/neurolang_data/raw/main/Julich-Brain/WB/22/MPM/'\n",
    "            'JulichBrain_MPMAtlas_r_N10_nlin2Stdicbm152asym2009c_publicDOI_14622b49a715338ce96e96611d395646.nii.gz',\n",
    "            {'move': 'wb22_r.nii.gz'}\n",
    "        )\n",
    "    ]\n",
    ")[0]\n",
    "\n",
    "img_r = image.load_img(wb22_r)\n",
    "img_l = image.load_img(wb22_l)\n",
    "img_l_data = img_l.get_fdata()\n",
    "img_r_data = img_r.get_fdata()\n",
    "img_l_unmaskes = np.nonzero(img_l_data)\n",
    "\n",
    "for v in zip(*img_l_unmaskes):\n",
    "    value = img_l_data[v[0]][v[1]][v[2]]\n",
    "    ex_value = img_r_data[v[0]][v[1]][v[2]]\n",
    "    if ex_value == 0:\n",
    "        img_r_data[v[0]][v[1]][v[2]] = value + 1000\n",
    "    \n",
    "conc_img = nib.spatialimages.SpatialImage(img_r_data, img_r.affine)\n",
    "\n",
    "conc_img = image.resample_img(\n",
    "    conc_img, mni_t1_4mm.affine, interpolation='nearest'\n",
    ")\n",
    "\n",
    "conc_img_data = conc_img.get_fdata()\n",
    "conc_img_unmaskes = np.nonzero(conc_img_data)\n",
    "\n",
    "julich_brain = []\n",
    "for v in zip(*conc_img_unmaskes):\n",
    "    julich_brain.append((v[0], v[1], v[2], conc_img_data[v[0]][v[1]][v[2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "respiratory-implement",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-784ca18fcaba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mns_database\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mijk_positions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mns_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns_features_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m ns_terms = (\n\u001b[1;32m     31\u001b[0m     pd.melt(\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas-1.1.5-py3.8-macosx-10.9-x86_64.egg/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas-1.1.5-py3.8-macosx-10.9-x86_64.egg/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas-1.1.5-py3.8-macosx-10.9-x86_64.egg/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas-1.1.5-py3.8-macosx-10.9-x86_64.egg/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas-1.1.5-py3.8-macosx-10.9-x86_64.egg/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# NeuroSynth\n",
    "ns_database_fn, ns_features_fn = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('neurosynth'),\n",
    "    [\n",
    "        (\n",
    "            'database.txt',\n",
    "            'https://github.com/neurosynth/neurosynth-data/raw/master/current_data.tar.gz',\n",
    "            {'uncompress': True}\n",
    "        ),\n",
    "        (\n",
    "            'features.txt',\n",
    "            'https://github.com/neurosynth/neurosynth-data/raw/master/current_data.tar.gz',\n",
    "            {'uncompress': True}\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ns_database = pd.read_csv(ns_database_fn, sep=f'\\t')\n",
    "ijk_positions = (\n",
    "    nib.affines.apply_affine(\n",
    "        np.linalg.inv(mni_t1_4mm.affine),\n",
    "        ns_database[['x', 'y', 'z']]\n",
    "    ).astype(int)\n",
    ")\n",
    "ns_database['i'] = ijk_positions[:, 0]\n",
    "ns_database['j'] = ijk_positions[:, 1]\n",
    "ns_database['k'] = ijk_positions[:, 2]\n",
    "\n",
    "ns_features = pd.read_csv(ns_features_fn, sep=f'\\t')\n",
    "ns_terms = (\n",
    "    pd.melt(\n",
    "            ns_features,\n",
    "            var_name='term', id_vars='pmid', value_name='TfIdf'\n",
    "       )\n",
    "    .query('TfIdf > 1e-3')[['pmid', 'term']]\n",
    ")\n",
    "ns_docs = ns_features[['pmid']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CogAt\n",
    "cogAt = datasets.utils._fetch_files(\n",
    "    datasets.utils._get_dataset_dir('CogAt'),\n",
    "    [\n",
    "        (\n",
    "            'cogat.xml',\n",
    "            'http://data.bioontology.org/ontologies/COGAT/download?'\n",
    "            'apikey=8b5b7825-538d-40e0-9e9e-5ab9274a9aeb&download_format=rdf',\n",
    "            {'move': 'cogat.xml'}\n",
    "        )\n",
    "    ]\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-macro",
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = NeurolangPDL()\n",
    "nl.load_ontology(cogAt)\n",
    "\n",
    "@nl.add_symbol\n",
    "def agg_max(i: Iterable) -> float:\n",
    "    return np.max(i)\n",
    "\n",
    "@nl.add_symbol\n",
    "def mean(iterable: Iterable) -> float:\n",
    "    return np.mean(iterable)\n",
    "\n",
    "\n",
    "@nl.add_symbol\n",
    "def std(iterable: Iterable) -> float:\n",
    "    return np.std(iterable)\n",
    "\n",
    "\n",
    "part_of = nl.new_symbol(name='http://www.obofoundry.org/ro/ro.owl#part_of')\n",
    "subclass_of = nl.new_symbol(name=str(RDFS.subClassOf))\n",
    "label = nl.new_symbol(name=str(RDFS.label))\n",
    "hasTopConcept = nl.new_symbol(name='http://www.w3.org/2004/02/skos/core#hasTopConcept')\n",
    "\n",
    "@nl.add_symbol\n",
    "def word_lower(name: str) -> str:\n",
    "    return name.lower()\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "\n",
    "ns_doc_folds = pd.concat(\n",
    "    ns_docs.iloc[train].assign(fold=[i] * len(train))\n",
    "    for i, (train, _) in enumerate(kfold.split(ns_docs))\n",
    ")\n",
    "doc_folds = nl.add_tuple_set(ns_doc_folds, name='doc_folds')\n",
    "\n",
    "\n",
    "activations = nl.add_tuple_set(ns_database.values, name='activations')\n",
    "terms = nl.add_tuple_set(ns_terms.values, name='terms')\n",
    "docs = nl.add_uniform_probabilistic_choice_over_set(\n",
    "        ns_docs.values, name='docs'\n",
    ")\n",
    "\n",
    "terms_det = nl.add_tuple_set(\n",
    "        ns_terms.term.unique(), name='terms_det'\n",
    ")\n",
    "\n",
    "j_brain = nl.add_tuple_set(\n",
    "    julich_brain,\n",
    "    name='julich_brain'\n",
    ")\n",
    "\n",
    "j_regions = nl.add_tuple_set(\n",
    "    regions.values,\n",
    "    name='julich_regions'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-karen",
   "metadata": {},
   "outputs": [],
   "source": [
    "with nl.scope as e:\n",
    "\n",
    "    e.ontology_terms[e.onto_name] = (\n",
    "    hasTopConcept[e.uri, e.cp] &\n",
    "    label[e.uri, e.onto_name]\n",
    "    )\n",
    "\n",
    "    e.lower_terms[e.term] = (\n",
    "        e.ontology_terms[e.onto_term] &\n",
    "        (e.term == word_lower[e.onto_term])\n",
    "    )\n",
    "\n",
    "    e.filtered_terms[e.d, e.t] = (\n",
    "        e.terms[e.d, e.t] &\n",
    "        e.lower_terms[e.t]\n",
    "    )\n",
    "\n",
    "    f_term = nl.solve_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = f_term['filtered_terms'].as_pandas_dataframe()\n",
    "filtered_terms = nl.add_tuple_set(filtered.values, name='filtered_terms')\n",
    "\n",
    "terms_det = nl.add_tuple_set(\n",
    "        filtered.t.unique(), name='terms_det'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-award",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = regions[regions.r_number != 103] #Remove amygdala (error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-raising",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os.path\n",
    "\n",
    "for name, id_region, _ in tqdm(regions.values):\n",
    "    print(f'{id_region} - {name}')\n",
    "    start_time = datetime.datetime.now()\n",
    "    if os.path.isfile('reverse_inference_results/neuro_paper_ri_no_term_probs_region{id_region}_{n_folds}folds.hdf'):\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(f'{id_region} - {name}: {end_time - start_time}')\n",
    "        print('--------------')\n",
    "        continue\n",
    "    \n",
    "    with nl.scope as e:\n",
    "\n",
    "        e.act_regions[e.d, id_region] = (\n",
    "            e.julich_brain[e.i, e.j, e.k, id_region] &\n",
    "            e.activations[\n",
    "                e.d, ..., ..., ..., ..., 'MNI', ..., ..., ..., ...,\n",
    "                ..., ..., ..., e.i, e.j, e.k\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        e.no_act_regions[e.d, e.id] = (\n",
    "            ~(e.act_regions[e.d, e.id]) &\n",
    "             e.doc_folds[e.d, ...] &\n",
    "            e.julich_regions[..., e.id]\n",
    "        )\n",
    "        \n",
    "        e.term_prob[e.t, e.fold, e.PROB[e.t, e.fold]] = (\n",
    "            (\n",
    "                e.filtered_terms[e.d, e.t]\n",
    "            ) // (\n",
    "                e.act_regions[e.d, id_region] &\n",
    "                e.doc_folds[e.d, e.fold] &\n",
    "                e.docs[e.d]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        e.no_term_prob[e.t, e.fold, e.PROB[e.t, e.fold]] = (\n",
    "           (\n",
    "                e.filtered_terms[e.d, e.t]\n",
    "            ) // (\n",
    "                e.no_act_regions[e.d, id_region] &\n",
    "                e.doc_folds[e.d, e.fold] &\n",
    "                e.docs[e.d]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        \n",
    "        res = nl.solve_all()\n",
    "        \n",
    "        end_time = datetime.datetime.now()\n",
    "        print(f'{id_region} - {name}: {end_time - start_time}')\n",
    "        \n",
    "        pss = res['term_prob'].as_pandas_dataframe()\n",
    "        \n",
    "        pss.to_hdf(f'reverse_inference_results/neuro_paper_ri_term_probs_region{id_region}_{n_folds}folds.hdf', key=f'results')\n",
    "        \n",
    "        pss = res['no_term_prob'].as_pandas_dataframe()\n",
    "        \n",
    "        pss.to_hdf(f'reverse_inference_results/neuro_paper_ri_no_term_probs_region{id_region}_{n_folds}folds.hdf', key=f'results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-cement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "neurolang",
   "language": "python",
   "name": "neurolang"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
